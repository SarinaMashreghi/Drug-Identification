The process is intuitively understandable. First step is tokenization, which transform sentence to tokens. Second is creating dictionary, which removes word duplication and make word set(which is called dictionary or vocabulary). Final step is counting occurrences of each words and make it Bag-of-Words model. As you can see, “likes” and “movies” show 2 as they appears two times in sample sentence. 